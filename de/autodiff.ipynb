{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2020 Die TensorFlow-Autoren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
        },
        "colab_type": "code",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [

      ],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Einführung in Gradienten und automatische Differenzierung"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfBg1C5NB3X0"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>     <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ansicht auf TensorFlow.org</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Führen Sie es in Google Colab aus</a>   </td>\n",
        "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Quelle auf GitHub ansehen</a>   </td>\n",
        "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Notizbuch herunterladen</a>   </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6P32iYYV27b"
      },
      "source": [
        "## Automatische Differenzierung und Farbverläufe\n",
        "\n",
        "[Die automatische Differenzierung](https://en.wikipedia.org/wiki/Automatic_differentiation) ist nützlich für die Implementierung von Algorithmen für maschinelles Lernen wie [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) zum Training neuronaler Netze.\n",
        "\n",
        "In diesem Leitfaden besprechen wir Möglichkeiten, wie Sie mit TensorFlow Farbverläufe berechnen können, insbesondere bei der Eager-Ausführung."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MUXex9ctTuDB"
      },
      "source": [
        "## Aufstellen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "IqR2PQG4ZaZ0"
      },
      "outputs": [

      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xHxb-dlhMIzW"
      },
      "source": [
        "## Berechnung von Farbverläufen\n",
        "\n",
        "Zur automatischen Unterscheidung muss sich TensorFlow merken, welche Vorgänge während des *Vorwärtsdurchlaufs* in welcher Reihenfolge stattfinden. Während des *Rückwärtsdurchlaufs* durchläuft TensorFlow dann diese Liste von Operationen in umgekehrter Reihenfolge, um Gradienten zu berechnen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1CLWJl0QliB0"
      },
      "source": [
        "## Farbverlaufsbänder\n",
        "\n",
        "TensorFlow stellt die [tf.GradientTape-](https://www.tensorflow.org/api_docs/python/tf/GradientTape) API zur automatischen Differenzierung bereit; Das heißt, die Berechnung des Gradienten einer Berechnung in Bezug auf einige Eingaben, normalerweise `tf.Variable` s. TensorFlow „zeichnet“ relevante Vorgänge, die im Kontext eines `tf.GradientTape` ausgeführt werden, auf einem „Band“ auf. TensorFlow verwendet dann dieses Band, um die Gradienten einer „aufgezeichneten“ Berechnung mithilfe [der Reverse-Mode-Differenzierung](https://en.wikipedia.org/wiki/Automatic_differentiation) zu berechnen.\n",
        "\n",
        "Hier ist ein einfaches Beispiel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Xq9GgTCP7a4A"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CR9tFAP_7cra"
      },
      "source": [
        "Sobald Sie einige Vorgänge aufgezeichnet haben, verwenden Sie `GradientTape.gradient(target, sources)` , um den Gradienten eines Ziels (oft ein Verlust) relativ zu einer Quelle (oft die Variablen des Modells) zu berechnen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "LsvrwF6bHroC"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2_aqsO25Vx1"
      },
      "source": [
        "Das obige Beispiel verwendet Skalare, aber `tf.GradientTape` funktioniert genauso einfach mit jedem Tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "vacZ3-Ws5VdV"
      },
      "outputs": [

      ],
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4eXOkrQ-9Pb"
      },
      "source": [
        "Um den Gradienten von `y` in Bezug auf beide Variablen zu erhalten, können Sie beide als Quellen an die `gradient` übergeben. Das Band ist hinsichtlich der Übergabe von Quellen flexibel und akzeptiert jede verschachtelte Kombination von Listen oder Wörterbüchern und gibt den Farbverlauf auf die gleiche Weise strukturiert zurück (siehe `tf.nest` )."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "luOtK1Da_BR0"
      },
      "outputs": [

      ],
      "source": [
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ei4iVXi6qgM7"
      },
      "source": [
        "Der Gradient in Bezug auf jede Quelle hat die Form der Quelle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "aYbWRFPZqk4U"
      },
      "outputs": [

      ],
      "source": [
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dI_SzxHsvao1"
      },
      "source": [
        "Hier ist noch einmal die Gradientenberechnung, diesmal mit Übergabe eines Variablenwörterbuchs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "d73cY6NOuaMd"
      },
      "outputs": [

      ],
      "source": [
        "my_vars = {\n",
        "    'w': tf.Variable(tf.random.normal((3, 2)), name='w'),\n",
        "    'b': tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HZ2LvHifEMgO"
      },
      "source": [
        "## Farbverläufe in Bezug auf ein Modell\n",
        "\n",
        "Es ist üblich, `tf.Variables` in einem `tf.Module` oder einer seiner Unterklassen ( `layers.Layer` , `keras.Model` ) für [Prüfpunkte](checkpoint.ipynb) und [Exporte](saved_model.ipynb) zu sammeln.\n",
        "\n",
        "In den meisten Fällen möchten Sie Gradienten in Bezug auf die trainierbaren Variablen eines Modells berechnen. Da alle Unterklassen von `tf.Module` ihre Variablen in der Eigenschaft `Module.trainable_variables` aggregieren, können Sie diese Verläufe in wenigen Codezeilen berechnen: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JvesHtbQESc-"
      },
      "outputs": [

      ],
      "source": [
        "layer = tf.keras.layers.Dense(2, activation='relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Forward pass\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# Calculate gradients with respect to every trainable variable\n",
        "grad = tape.gradient(loss, layer.trainable_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "PR_ezr6UFrpI"
      },
      "outputs": [

      ],
      "source": [
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f'{var.name}, shape: {g.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6Gx6LS714zR"
      },
      "source": [
        "<a id=\"watches\"></a>\n",
        "\n",
        "## Kontrollieren, was das Band sieht"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N4VlqKFzzGaC"
      },
      "source": [
        "Das Standardverhalten besteht darin, alle Vorgänge nach dem Zugriff auf eine trainierbare `tf.Variable` aufzuzeichnen. Die Gründe hierfür sind:\n",
        "\n",
        "- Das Band muss wissen, welche Vorgänge im Vorwärtsdurchlauf aufgezeichnet werden müssen, um die Steigungen im Rückwärtsdurchlauf berechnen zu können.\n",
        "- Das Band enthält Verweise auf Zwischenausgaben, sodass Sie keine unnötigen Vorgänge aufzeichnen möchten.\n",
        "- Der häufigste Anwendungsfall besteht darin, den Gradienten eines Verlusts in Bezug auf alle trainierbaren Variablen eines Modells zu berechnen.\n",
        "\n",
        "Im Folgenden kann beispielsweise kein Gradient berechnet werden, da der `tf.Tensor` standardmäßig nicht „überwacht“ wird und die `tf.Variable` nicht trainierbar ist:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "Kj9gPckdB37a"
      },
      "outputs": [

      ],
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "# Not a variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RkcpQnLgNxgi"
      },
      "source": [
        "Mit der Methode `GradientTape.watched_variables` können Sie die vom Band überwachten Variablen auflisten:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "hwNwjW1eAkib"
      },
      "outputs": [

      ],
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NB9I1uFvB4tf"
      },
      "source": [
        "`tf.GradientTape` bietet Hooks, die dem Benutzer die Kontrolle darüber geben, was beobachtet wird und was nicht.\n",
        "\n",
        "Um Farbverläufe in Bezug auf einen `tf.Tensor` aufzuzeichnen, müssen Sie `GradientTape.watch(x)` aufrufen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "tVN1QqFRDHBK"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qxsiYnf2DN8K"
      },
      "source": [
        "Um umgekehrt das Standardverhalten der Überwachung aller `tf.Variables` zu deaktivieren, legen Sie beim Erstellen des Verlaufsbands `watch_accessed_variables=False` fest. Diese Berechnung verwendet zwei Variablen, verbindet jedoch nur den Gradienten für eine der Variablen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7QPzwWvSEwIp"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x1)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TRduLbE1H2IJ"
      },
      "source": [
        "Da `GradientTape.watch` nicht für `x0` aufgerufen wurde, wird diesbezüglich kein Gradient berechnet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e6GM-3evH1Sz"
      },
      "outputs": [

      ],
      "source": [
        "# dy = 2x * dx\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'])\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2g1nKB6P-OnA"
      },
      "source": [
        "## Zwischenergebnisse\n",
        "\n",
        "Sie können auch Farbverläufe der Ausgabe in Bezug auf Zwischenwerte anfordern, die im `tf.GradientTape` Kontext berechnet werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "7XaPRAwUyYms"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "# Use the tape to compute the gradient of z with respect to the\n",
        "# intermediate value y.\n",
        "# dz_dx = 2 * y, where y = x ** 2\n",
        "print(tape.gradient(z, y).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ISkXuY7YzIcS"
      },
      "source": [
        "Standardmäßig werden die von einem `GradientTape` gehaltenen Ressourcen freigegeben, sobald die Methode `GradientTape.gradient()` aufgerufen wird. Um mehrere Farbverläufe im Rahmen derselben Berechnung zu berechnen, erstellen Sie ein `persistent` Farbverlaufsband. Dies ermöglicht mehrere Aufrufe der Methode `gradient()` , da Ressourcen freigegeben werden, wenn das Bandobjekt durch Garbage Collection erfasst wird. Zum Beispiel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "zZaCm3-9zVCi"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant([1, 3.0])\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
        "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "j8bv_jQFg6CN"
      },
      "outputs": [

      ],
      "source": [
        "del tape   # Drop the reference to the tape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_ZY-9BUB7vX"
      },
      "source": [
        "## Hinweise zur Leistung\n",
        "\n",
        "- Mit der Ausführung von Vorgängen innerhalb eines Verlaufsbandkontexts ist ein geringfügiger Mehraufwand verbunden. Für die meisten eifrigen Ausführungen wird dies keine nennenswerten Kosten darstellen, aber Sie sollten Bandkontext trotzdem nur in den Bereichen verwenden, in denen es erforderlich ist.\n",
        "\n",
        "- Verlaufsbänder verwenden Speicher zum Speichern von Zwischenergebnissen, einschließlich Eingaben und Ausgaben, zur Verwendung während des Rückwärtsdurchlaufs.\n",
        "\n",
        "    Aus Effizienzgründen müssen einige Operationen (wie `ReLU` ) ihre Zwischenergebnisse nicht behalten und werden während des Vorwärtsdurchlaufs bereinigt. Wenn Sie jedoch `persistent=True` auf Ihrem Band verwenden, *wird nichts verworfen* und Ihre Spitzenspeicherauslastung wird höher sein."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9dLBpZsJebFq"
      },
      "source": [
        "## Gradienten nichtskalarer Ziele"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7pldU9F5duP2"
      },
      "source": [
        "Ein Gradient ist grundsätzlich eine Operation auf einem Skalar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "qI0sDV_WeXBb"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient(y0, x).numpy())\n",
        "print(tape.gradient(y1, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "COEyYp34fxj4"
      },
      "source": [
        "Wenn Sie also nach dem Gradienten mehrerer Ziele fragen, ist das Ergebnis für jede Quelle:\n",
        "\n",
        "- Der Gradient der Summe der Ziele oder gleichwertig\n",
        "- Die Summe der Gradienten jedes Ziels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "o4a6_YOcfWKS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uvP-mkBMgbym"
      },
      "source": [
        "Wenn das/die Ziel(e) nicht skalar sind, wird auf ähnliche Weise der Gradient der Summe berechnet:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "DArPWqsSh5un"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3., 4.]\n",
        "\n",
        "print(tape.gradient(y, x).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "flDbx68Zh5Lb"
      },
      "source": [
        "Dies macht es einfach, den Gradienten der Summe einer Sammlung von Verlusten oder den Gradienten der Summe einer elementweisen Verlustberechnung zu ermitteln.\n",
        "\n",
        "Wenn Sie für jedes Element einen separaten Farbverlauf benötigen, sehen Sie sich [Jacobians](advanced_autodiff.ipynb#jacobians) an."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iwFswok8RAly"
      },
      "source": [
        "In manchen Fällen können Sie den Jacobi überspringen. Bei einer elementweisen Berechnung ergibt der Gradient der Summe die Ableitung jedes Elements nach seinem Eingabeelement, da jedes Element unabhängig ist:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "JQvk_jnMmTDS"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.linspace(-10.0, 10.0, 200+1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x)\n",
        "\n",
        "dy_dx = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "e_f2QgDPmcPE"
      },
      "outputs": [

      ],
      "source": [
        "plt.plot(x, y, label='y')\n",
        "plt.plot(x, dy_dx, label='dy/dx')\n",
        "plt.legend()\n",
        "_ = plt.xlabel('x')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6kADybtQzYj4"
      },
      "source": [
        "## Kontrollfluss\n",
        "\n",
        "Da Bänder Operationen aufzeichnen, während sie ausgeführt werden, wird der Python-Kontrollfluss (z. B. mit `if` s und `while` s) auf natürliche Weise gehandhabt.\n",
        "\n",
        "Hier wird in jedem Zweig eines `if` eine andere Variable verwendet. Der Farbverlauf verbindet sich nur mit der Variablen, die verwendet wurde:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "ciFLizhrrjy7"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.constant(1.0)\n",
        "\n",
        "v0 = tf.Variable(2.0)\n",
        "v1 = tf.Variable(2.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  if x > 0.0:\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1**2 \n",
        "\n",
        "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0)\n",
        "print(dv1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKnLaiapsjeP"
      },
      "source": [
        "Denken Sie daran, dass die Steueranweisungen selbst nicht differenzierbar sind und daher für verlaufsbasierte Optimierer unsichtbar sind.\n",
        "\n",
        "Abhängig vom Wert von `x` im obigen Beispiel zeichnet das Band entweder `result = v0` oder `result = v1**2` auf. Der Gradient in Bezug auf `x` ist immer `None` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "8k05WmuAwPm7"
      },
      "outputs": [

      ],
      "source": [
        "dx = tape.gradient(result, x)\n",
        "\n",
        "print(dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "egypBxISAHhx"
      },
      "source": [
        "## Einen Farbverlauf von `None` erhalten\n",
        "\n",
        "Wenn ein Ziel nicht mit einer Quelle verbunden ist, erhalten Sie den Farbverlauf `None` .\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "CU185WDM81Ut"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y * y\n",
        "print(tape.gradient(z, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZbKpHfBRJym"
      },
      "source": [
        "Hier ist `z` offensichtlich nicht mit `x` verbunden, aber es gibt mehrere weniger offensichtliche Möglichkeiten, wie ein Gradient getrennt werden kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eHDzDOiQ8xmw"
      },
      "source": [
        "### 1. Eine Variable durch einen Tensor ersetzt.\n",
        "\n",
        "Im Abschnitt [„Steuern, was das Band überwacht“](#watches) haben Sie gesehen, dass das Band automatisch eine `tf.Variable` , aber keinen `tf.Tensor` überwacht.\n",
        "\n",
        "Ein häufiger Fehler besteht darin, versehentlich eine `tf.Variable` durch eine `tf.Tensor` zu ersetzen, anstatt `Variable.assign` zum Aktualisieren der `tf.Variable` zu verwenden. Hier ist ein Beispiel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "QPKY4Tn9zX7_"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable(2.0)\n",
        "\n",
        "for epoch in range(2):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = x+1\n",
        "\n",
        "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
        "  x = x + 1   # This should be `x.assign_add(1)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3gwZKxgA97an"
      },
      "source": [
        "### 2. Berechnungen außerhalb von TensorFlow durchgeführt\n",
        "\n",
        "Das Band kann den Verlaufspfad nicht aufzeichnen, wenn die Berechnung TensorFlow verlässt. Zum Beispiel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "jmoLCDJb_yw1"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x2 = x**2\n",
        "\n",
        "  # This step is calculated with NumPy\n",
        "  y = np.mean(x2, axis=0)\n",
        "\n",
        "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
        "  # using `tf.convert_to_tensor`.\n",
        "  y = tf.reduce_mean(y, axis=0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p3YVfP3R-tp7"
      },
      "source": [
        "### 3. Verläufe durch eine Ganzzahl oder einen String erstellt\n",
        "\n",
        "Ganzzahlen und Zeichenfolgen sind nicht differenzierbar. Wenn ein Berechnungspfad diese Datentypen verwendet, gibt es keinen Gradienten.\n",
        "\n",
        "Niemand erwartet, dass Zeichenfolgen differenzierbar sind, aber es ist leicht, versehentlich eine `int` Konstante oder -Variable zu erstellen, wenn Sie den `dtype` nicht angeben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "9jlHXHqfASU3"
      },
      "outputs": [

      ],
      "source": [
        "# The x0 variable has an `int` dtype.\n",
        "x = tf.Variable([[2, 2],\n",
        "                 [2, 2]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # The path to x1 is blocked by the `int` dtype here.\n",
        "  y = tf.cast(x, tf.float32)\n",
        "  y = tf.reduce_sum(x)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RsdP_mTHX9L1"
      },
      "source": [
        "TensorFlow führt keine automatische Umwandlung zwischen Typen durch, daher erhalten Sie in der Praxis häufig einen Typfehler anstelle eines fehlenden Farbverlaufs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyAZ7C8qCEs6"
      },
      "source": [
        "### 5. Verläufe durch ein zustandsbehaftetes Objekt aufgenommen\n",
        "\n",
        "Staat stoppt Steigungen. Wenn Sie von einem zustandsbehafteten Objekt lesen, kann das Band nur den aktuellen Status sehen, nicht den Verlauf, der dazu geführt hat.\n",
        "\n",
        "Ein `tf.Tensor` ist unveränderlich. Sie können einen Tensor nicht mehr ändern, sobald er erstellt wurde. Es hat einen *Wert* , aber keinen *Zustand* . Alle bisher besprochenen Operationen sind zudem zustandslos: Die Ausgabe eines `tf.matmul` hängt nur von seinen Eingaben ab.\n",
        "\n",
        "Eine `tf.Variable` hat einen internen Status, ihren Wert. Wenn Sie die Variable verwenden, wird der Status gelesen. Es ist normal, einen Gradienten in Bezug auf eine Variable zu berechnen, aber der Zustand der Variablen verhindert, dass Gradientenberechnungen weiter zurückgehen. Zum Beispiel:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "C1tLeeRFE479"
      },
      "outputs": [

      ],
      "source": [
        "x0 = tf.Variable(3.0)\n",
        "x1 = tf.Variable(0.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # Update x1 = x1 + x0.\n",
        "  x1.assign_add(x0)\n",
        "  # The tape starts recording from x1.\n",
        "  y = x1**2   # y = (x1 + x0)**2\n",
        "\n",
        "# This doesn't work.\n",
        "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xKA92-dqF2r-"
      },
      "source": [
        "Ebenso sind `tf.data.Dataset` Iteratoren und `tf.queue` s zustandsbehaftet und stoppen alle Gradienten auf Tensoren, die sie durchlaufen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HHvcDGIbOj2I"
      },
      "source": [
        "## Kein Gefälle registriert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aoc-A6AxVqry"
      },
      "source": [
        "Einige `tf.Operation` s sind **als nicht differenzierbar registriert** und geben `None` zurück. Bei anderen ist **kein Gefälle registriert** .\n",
        "\n",
        "Auf der Seite [„tf.raw_ops“](https://www.tensorflow.org/api_docs/python/tf/raw_ops) wird angezeigt, bei welchen Operationen auf niedriger Ebene Farbverläufe registriert sind.\n",
        "\n",
        "Wenn Sie versuchen, einen Farbverlauf durch eine Float-Operation zu übertragen, für die kein Farbverlauf registriert ist, gibt das Band einen Fehler aus, anstatt stillschweigend `None` zurückzugeben. Auf diese Weise wissen Sie, dass etwas schief gelaufen ist.\n",
        "\n",
        "Beispielsweise umschließt die Funktion `tf.image.adjust_contrast` [raw_ops.AdjustContrastv2](https://www.tensorflow.org/api_docs/python/tf/raw_ops#.AdjustContrastv2) , das einen Farbverlauf haben könnte, der Farbverlauf jedoch nicht implementiert ist:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "HSb20FXc_V0U"
      },
      "outputs": [

      ],
      "source": [
        "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
        "delta = tf.Variable(0.1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  new_image = tf.image.adjust_contrast(image, delta)\n",
        "\n",
        "try:\n",
        "  print(tape.gradient(new_image, [image, delta]))\n",
        "  assert False   # This should not happen.\n",
        "except LookupError as e:\n",
        "  print(f'{type(e).__name__}: {e}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pDoutjzATiEm"
      },
      "source": [
        "Wenn Sie durch diese Operation differenzieren müssen, müssen Sie entweder den Farbverlauf implementieren und registrieren (mit `tf.RegisterGradient` ) oder die Funktion mithilfe anderer Operationen erneut implementieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GCTwc_dQXp2W"
      },
      "source": [
        "## Nullen statt Keine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TYDrVogA89eA"
      },
      "source": [
        "In einigen Fällen wäre es praktisch, für nicht verbundene Farbverläufe 0 anstelle von `None` zu erhalten. Mit dem Argument `unconnected_gradients` können Sie entscheiden, was zurückgegeben werden soll, wenn Sie nicht verbundene Farbverläufe haben:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
        },
        "colab_type": "code",
        "id": "U6zxk1sf9Ixx"
      },
      "outputs": [

      ],
      "source": [
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Tce3stUlHN0L"
      ],
      "name": "autodiff.ipynb",
      "private_outputs": true,
      "provenance": [

      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
